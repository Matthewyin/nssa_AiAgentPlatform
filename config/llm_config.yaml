# LLM 配置
# 注意：所有 LLM 相关配置都应该在此文件中定义
# base_url 可以使用环境变量 ${OLLAMA_BASE_URL}，如果环境变量不存在则使用默认值

llm:
  # 当前使用的 Provider，可选：ollama / openai / gemini / deepseek
  # provider: "ollama"
  # provider: "gemini"
  provider: "deepseek"

  # 通用参数（所有 Provider 共用）
  # model: "deepseek-reasoner"
  # model: "gemini-2.5-pro"
  # model: "deepseek-r1:8b"
  
  # deepseek建议生成代码时为0，数据分析为1，
  # temperature: 1 

  # 其他大模型温度设置
  temperature: 1
  max_tokens: 10240
  timeout: 120

  # Provider 通用字段（由下方 providers.* 预设或这里覆盖）
  # 注意：base_url 应该在各 provider 配置中单独设置，不要在这里设置通用值
  # base_url: "${OLLAMA_BASE_URL}"

# 各 Provider 预设配置（可选，用于提供默认 model/base_url）
providers:
  ollama:
    base_url: "${OLLAMA_BASE_URL}"
    model: "deepseek-r1:8b"

  # OpenAI / ChatGPT
  openai:
    # 注意：API Key 请通过系统环境变量 OPENAI_API_KEY 提供
    # base_url 可选使用 OPENAI_BASE_URL 或在此覆盖
    api_key: "${OPENAI_API_KEY}"
    base_url: "${OPENAI_BASE_URL}"
    model: "gpt-5.1"

  # Google Gemini
  gemini:
    # 注意：API Key 请通过系统环境变量 GEMINI_API_KEY 提供
    api_key: "${GEMINI_API_KEY}"
    model: "gemini-2.5-flash"
    # Gemini 不需要 base_url，使用官方默认地址
    # 注意：RAG 配置已移至独立的 gemini_rag 节（见文件末尾）

  # DeepSeek（OpenAI 兼容接口）
  deepseek:
    # 注意：API Key 请通过系统环境变量 DEEPSEEK_API_KEY 提供
    api_key: "${DEEPSEEK_API_KEY}"
    base_url: "${DEEPSEEK_BASE_URL}"
    model: "deepseek-reasoner"
    # model: "deepseek-chat"

# Gemini RAG 专用配置（独立于 LLM provider）
# 用于知识库检索，可以与主 LLM provider 不同
gemini_rag:
  # API Key 可以与 gemini provider 共用，也可以单独配置
  api_key: "${GEMINI_API_KEY}"
  # RAG 模型，建议使用 flash 版本以节省配额
  # gemini-2.5-flash: 每分钟 15 次，每天 1500 次
  # gemini-2.5-pro: 每分钟 5 次，每天 25 次（配额很低！）
  model: "gemini-2.5-flash"

# Embedding 配置（保持与之前兼容）
embedding:
  provider: "ollama"
  base_url: "${OLLAMA_BASE_URL}"  # 从环境变量读取，默认: http://localhost:11434
  model: "nomic-embed-text"  # Ollama 嵌入模型
